{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hfDTUh9zOd3"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") # Ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hSR6mV8xIWO"
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def load_data(path):\n",
    "    # uORB topics used\n",
    "    uorb_topics = ( \n",
    "        'vehicle_gps_position_0.csv',\n",
    "        'vehicle_local_position_0.csv',\n",
    "        'vehicle_attitude_setpoint_0.csv',\n",
    "        'rate_ctrl_status_0.csv',\n",
    "        'vehicle_attitude_0.csv',\n",
    "        'vehicle_angular_velocity_0.csv',\n",
    "        'vehicle_magnetometer_0.csv',\n",
    "        'vehicle_air_data_0.csv',\n",
    "        'battery_status_0.csv'\n",
    "    )\n",
    "    # Create dfs list\n",
    "    dfs = list()\n",
    "\n",
    "    for filename in sorted(os.listdir(path)): # Select the .csv to load\n",
    "        if filename.endswith(uorb_topics):\n",
    "            df = pd.read_csv(path + filename) # Read .csv\n",
    "\n",
    "            # Selected features\n",
    "            if filename.endswith(uorb_topics[0]): # vehicle_gps_position OK\n",
    "                df = df[['timestamp', 'lat', 'lon', 'alt', 'eph', 'epv', 'hdop', 'vdop', 'noise_per_ms', \n",
    "                         'jamming_indicator', 'vel_m_s', 'cog_rad']]\n",
    "            elif filename.endswith(uorb_topics[1]): # vehicle_local_position\n",
    "                 df = df[['timestamp', 'x', 'y', 'z', 'vx', 'vy', 'vz', 'z_deriv', 'ax', 'ay', 'az', \n",
    "                          'heading', 'evh']]\n",
    "            elif filename.endswith(uorb_topics[2]): # vehicle_attitude_setpoint 'ACELERATION OK'\n",
    "                df = df[['timestamp', 'roll_body', 'pitch_body', 'yaw_body', 'yaw_sp_move_rate', 'thrust_body[2]']]\n",
    "            elif filename.endswith(uorb_topics[3]): # rate_ctrl_status\n",
    "                df = df[['timestamp', 'rollspeed_integ', 'pitchspeed_integ', 'yawspeed_integ']]\n",
    "            elif filename.endswith(uorb_topics[4]): # vehicle_attitude\n",
    "                df = df[['timestamp', 'q[0]', 'q[1]', 'q[2]', 'q[3]']]\n",
    "            elif filename.endswith(uorb_topics[5]): # vehicle_angular_velocity\n",
    "                df = df[['timestamp', 'xyz[0]', 'xyz[1]', 'xyz[2]']]\n",
    "            elif filename.endswith(uorb_topics[6]): # vehicle_magnetometer OK\n",
    "                df = df[['timestamp', 'magnetometer_ga[0]', 'magnetometer_ga[1]', 'magnetometer_ga[2]']]\n",
    "            elif filename.endswith(uorb_topics[7]): # vehicle_air_data\n",
    "                df = df[['timestamp', 'baro_pressure_pa', 'baro_alt_meter']]\n",
    "            elif filename.endswith(uorb_topics[8]): # battery_status\n",
    "                df = df[['timestamp', 'temperature']]\n",
    "            \n",
    "            # Drop constant columns\n",
    "            df = df.loc[:, df.apply(pd.Series.nunique) != 1]\n",
    "            # Drop all the NaN rows\n",
    "            df.dropna(inplace=True)\n",
    "            # Reset DataFrame index\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            if not df.empty: # Check if DataFrame is not empty\n",
    "                dfs.append(df) # Add DataFrame to list\n",
    "\n",
    "    # Return dfs list\n",
    "    return dfs\n",
    "\n",
    "\n",
    "\n",
    "def find_min_max_timestamp(dfs):\n",
    "    # Minimum and maximum timestamp of the first DataFrame\n",
    "    min = dfs[0][['timestamp']].min()[0]\n",
    "    max = dfs[0][['timestamp']].max()[0]\n",
    "\n",
    "    for df in dfs[1:]: # Loop through DataFrame list\n",
    "        # Compare if any DataFrame timestamp is less than min variable\n",
    "        if df[['timestamp']].min()[0] < min:\n",
    "            min = df[['timestamp']].min()[0] # Update min timestamp\n",
    "        \n",
    "        # Compare if any DataFrame timestamp is greater than max variable\n",
    "        if df[['timestamp']].max()[0] > max:\n",
    "            max = df[['timestamp']].max()[0] # Update max timestamp\n",
    "        \n",
    "    # Return minimum and maximum timestamp\n",
    "    return min, max\n",
    "\n",
    "\n",
    "\n",
    "def calculate_ideal_time_window(dfs_list):\n",
    "    acc_1 = 0 # Accumulator (1) of the mean values ​​of the time window of each list of DataFrame list\n",
    "    for dfs in dfs_list: # Loop through list of DataFrame list \n",
    "        time_window_avg = 0 # DataFrame mean time window\n",
    "\n",
    "        for df in dfs: # Loop through DataFrame list\n",
    "            acc_2 = 0 # Accumulator (2) of the subtraction of each timestamp\n",
    "\n",
    "            for i in range(len(df['timestamp'].to_list())): # Number of timestamp in DataFrame\n",
    "                try: # Try subtracting timestamps\n",
    "                    sub = df['timestamp'][i+1] - df['timestamp'][i]\n",
    "                    acc_2 += sub # Subtraction\n",
    "                except KeyError: # If it reaches the last index\n",
    "                    time_window_avg += acc_2 / df.shape[0] # Mean time window in DataFrame\n",
    "        \n",
    "        # Mean time window in DataFrame list\n",
    "        acc_1 += time_window_avg / len(dfs)\n",
    "\n",
    "    # Return rounded ideal time window\n",
    "    return round((acc_1 / len(dfs_list)) / 100000) * 100000\n",
    "\n",
    "\n",
    "\n",
    "def merge_dfs(dfs, time_window=200000): # Ideal time window already inserted\n",
    "    columns = [] # Empty columns list\n",
    "\n",
    "    for df in dfs: # Loop through DataFrame list\n",
    "        columns += list(df.columns) # Add columns to the list\n",
    "\n",
    "    columns = pd.unique(columns).tolist() # Remove duplicate values ​​from list\n",
    "    merge_df = pd.DataFrame(columns=columns) # Generate empty merge DataFrame\n",
    "\n",
    "    # Minimum and maximum timestamp\n",
    "    min_timestamp, max_timestamp = find_min_max_timestamp(dfs)\n",
    "    timestamp = min_timestamp # First timestamp is the minimum timestamp\n",
    "    timestamps = [] # Empty timestamp list\n",
    "\n",
    "    # Generate timestamps based on time window\n",
    "    while timestamp < max_timestamp:\n",
    "        timestamps.append(timestamp) # Add timestamp to list\n",
    "        timestamp += time_window # Sum timestamp with time window\n",
    "    else: # timestamp >= max_timestamp\n",
    "        timestamps.append(max_timestamp) # Last timestamp is the maximum timestamp\n",
    "    \n",
    "    merge_df.loc[:, 'timestamp'] = timestamps # Add timestamps with time window\n",
    "\n",
    "    for df in dfs: # Loop through DataFrame list\n",
    "        values = [] # Empty values\n",
    "\n",
    "        for i in range(len(timestamps)): # # Number of timestamp in merge DataFrame\n",
    "            try: # Group the data using the mean\n",
    "                mean_df = list(df[(df['timestamp'] >= merge_df['timestamp'][i]) & \n",
    "                                  (df['timestamp'] < merge_df['timestamp'][i + 1])].mean())[1:]\n",
    "                values.append(mean_df) # Add in values\n",
    "            except KeyError: # If it reaches the last index\n",
    "                mean_df = list(df[df['timestamp'] >= merge_df['timestamp'][i]].mean())[1:]\n",
    "                values.append(mean_df) # Add in values\n",
    "\n",
    "        # Add new values to the merge DataFrame\n",
    "        merge_df.loc[:, list(df.columns)[1:]] = values\n",
    "    \n",
    "    # Interpolation with the nearest method to fill NaN values\n",
    "    merge_df.interpolate(method='nearest', inplace=True)\n",
    "    # Drop all the NaN rows\n",
    "    merge_df.dropna(inplace=True)\n",
    "    # Return merge DataFrame\n",
    "    return merge_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "def merge_single_df(benign_df, jamming_df, spoofing_df):\n",
    "    # Add class column in each DataFrame\n",
    "    benign_df['class'] = 0 # Benign\n",
    "    jamming_df['class'] = 1 # GPS Jamming\n",
    "    spoofing_df['class'] = 2 # GPS Spoofing\n",
    "\n",
    "    # Concatenate the three DataFranes\n",
    "    df = pd.concat([benign_df, jamming_df, spoofing_df]).reset_index(drop=True)\n",
    "    # Drop all the NaN columns\n",
    "    df.dropna(axis=1, inplace=True)\n",
    "    # Return Train DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def split_x_y(df, columns_to_drop):\n",
    "    # Columns to drop\n",
    "    df = df.drop(columns=columns_to_drop)   \n",
    "    # DataFrame X (features)\n",
    "    X = df.loc[:, df.columns != 'class']\n",
    "    y = df.loc[:, 'class'] # y (labels)\n",
    "    # Return DataFrame X and y\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def normalize_data(df):\n",
    "    # Split into X and y\n",
    "    X, y = split_x_y(df, ['timestamp'])\n",
    "    # Apply normalization\n",
    "    # Timestamp is not normalized\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    # Normalized data\n",
    "    norm_df = pd.DataFrame(X_norm, columns=X.columns)\n",
    "    # Update DataFrame with normalized data\n",
    "    df = df[['class', 'timestamp']].join(norm_df)\n",
    "    # Return normalized DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def drop_highly_correlated_features(X):\n",
    "    # Create correlation matrix\n",
    "    corr_matrix = X.corr().abs()\n",
    "    # Select upper traingle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    # Find index of columns with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "    # Drop the columns\n",
    "    X.drop(columns=list(to_drop), axis=1, inplace=True)\n",
    "    # Return X after dropping columns with high correlation\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bo7E-DjZsbpM"
   },
   "outputs": [],
   "source": [
    "# Main\n",
    "\n",
    "# Dataset path\n",
    "dataset_path = '/home/leandro/remy-project/centralized/datasets/UAVGPSAttacks/'\n",
    "\n",
    "# Path of each category\n",
    "benign_path = dataset_path + 'Benign Flight/'\n",
    "jamming_path = dataset_path + 'GPS Jamming/'\n",
    "spoofing_path = dataset_path + 'GPS Spoofing/'\n",
    "\n",
    "# DataFrames list\n",
    "benign_dfs = []\n",
    "jamming_dfs = []\n",
    "spoofing_dfs = []\n",
    "\n",
    "# Load data of each class\n",
    "benign_dfs = load_data(benign_path)\n",
    "jamming_dfs = load_data(jamming_path)\n",
    "spoofing_dfs = load_data(spoofing_path)\n",
    "\n",
    "# Display ideal time window\n",
    "print('Ideal Time Window: ' + str(calculate_ideal_time_window([benign_dfs, jamming_dfs, spoofing_dfs])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvW6dWBg5d4e"
   },
   "outputs": [],
   "source": [
    "# Merge DataFrames\n",
    "benign_df = merge_dfs(benign_dfs)\n",
    "jamming_df = merge_dfs(jamming_dfs)\n",
    "spoofing_df = merge_dfs(spoofing_dfs)\n",
    "\n",
    "# Drop DataFrames list (no used)\n",
    "del benign_dfs, jamming_dfs, spoofing_dfs\n",
    "\n",
    "# Merge into a single DataFrame\n",
    "uav_df = merge_single_df(benign_df, jamming_df, spoofing_df)\n",
    "# Save data without normalizing (for analysis)\n",
    "uav_df.to_csv(f'{dataset_path}data.csv', index=False)\n",
    "# Normalize the data\n",
    "uav_df = normalize_data(uav_df)\n",
    "\n",
    "# Split into X and Y\n",
    "X, y = split_x_y(uav_df, [])\n",
    "# Drop columns with high correlation\n",
    "X = drop_highly_correlated_features(X)\n",
    "\n",
    "X['class'] = y.values # Add class column in X DataFrame\n",
    "# Save dataset in .csv\n",
    "X.to_csv(f'{dataset_path}data_norm.csv', index=False)\n",
    "# The next step is to apply the data to the unsupervised algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO2VNC7iNRoW06SoWvNfh/q",
   "provenance": [
    {
     "file_id": "1K3FjSlC7rIJEss1NauCnZlPYzipBMYri",
     "timestamp": 1673432920944
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
