{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7gBFkbd0TAH"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from imblearn.over_sampling import KMeansSMOTE, RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") # Ignore warnings\n",
    "RANDOM_STATE = 42 # Random state default\n",
    "# Set random seed in Keras\n",
    "keras.utils.set_random_seed(RANDOM_STATE)\n",
    "# Set random seed in NumPy\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHNdCn9m_xit"
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def split_train_test(df, size, time_column=' Time'):\n",
    "    # Build test DataFrame\n",
    "    test_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    for i in np.sort(pd.unique(df['class'])): # For each class\n",
    "        temp_df = df[df['class'] == i] # Select only data from a class\n",
    "        # Obtain a percentage of data (end of DataFrame)\n",
    "        temp_df = temp_df.tail(round(len(temp_df) * size))\n",
    "        # Drop data obtained from the training DataFrame\n",
    "        df.drop(index=temp_df.index, inplace=True)\n",
    "        # Add data in test DataFrame\n",
    "        test_df = pd.concat([test_df, temp_df])\n",
    "\n",
    "    # Sort by time column and reset index\n",
    "    # df is training (and validating) DataFrame\n",
    "    df = df.sort_values(by=[time_column]).reset_index(drop=True)\n",
    "    test_df = test_df.sort_values(by=[time_column]).reset_index(drop=True)\n",
    "    # Return df and test_df excluding time column\n",
    "    return df.drop(columns=[time_column]), test_df.drop(columns=[time_column])\n",
    "\n",
    "\n",
    "\n",
    "def split_x_y(df):    \n",
    "    # DataFrame X (features)\n",
    "    X = df.loc[:, df.columns != 'class']\n",
    "    y = df.loc[:, 'class'] # y (labels)\n",
    "    # DataFrame X and y\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def normalize_data(df):\n",
    "    # Split into X and y\n",
    "    X, y = split_x_y(df)\n",
    "    # Apply normalization\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    # Normalized data\n",
    "    norm_df = pd.DataFrame(X_norm, columns=df.drop(columns=['class']).columns)\n",
    "    # Update DataFrame with normalized data\n",
    "    df = df[['class']].join(norm_df)\n",
    "    # Return normalized DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def generate_gans(X_minor_train, n_features, coding_size):\n",
    "    # Build the generator\n",
    "    generator = keras.models.Sequential([\n",
    "        keras.layers.Dense(100, activation='selu', input_shape=[coding_size]),\n",
    "        keras.layers.Dense(200, activation='selu'),\n",
    "        keras.layers.Dense(300, activation='selu'),\n",
    "        keras.layers.Dense(400, activation='selu'),\n",
    "        keras.layers.Dense(500, activation='selu'),\n",
    "        keras.layers.Dense(n_features, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Build the discriminator\n",
    "    discriminator = keras.models.Sequential([\n",
    "        keras.layers.Dense(n_features),\n",
    "        keras.layers.Dense(500, activation='selu'),\n",
    "        keras.layers.Dense(400, activation='selu'),\n",
    "        keras.layers.Dense(300, activation='selu'),\n",
    "        keras.layers.Dense(200, activation='selu'),\n",
    "        keras.layers.Dense(100, activation='selu'),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Build a GAN\n",
    "    gan = keras.models.Sequential([generator, discriminator])\n",
    "    # Compile the discriminator\n",
    "    discriminator.compile(loss='binary_crossentropy',\n",
    "                        optimizer=keras.optimizers.Adam(learning_rate=10 ** -4))\n",
    "    # Freeze the discriminator\n",
    "    discriminator.trainable = False\n",
    "    # Compile the generator\n",
    "    gan.compile(loss='binary_crossentropy',\n",
    "                optimizer=keras.optimizers.Adam(learning_rate=10 ** -4))\n",
    "\n",
    "    # Get the generator and discriminator\n",
    "    generator, discriminator = gan.layers\n",
    "    # Get the indices of the training data of the minority class\n",
    "    idxs_minor_train = np.array(range(X_minor_train.shape[0]))\n",
    "    # The batch size\n",
    "    batch_size = 32\n",
    "    # Get the number of mini-batches\n",
    "    n_batch = len(idxs_minor_train) // batch_size\n",
    "    # The number of maximum epoch\n",
    "    max_iter = 10\n",
    "\n",
    "    for _ in range(max_iter): # For each epoch\n",
    "        # Shuffle the data\n",
    "        np.random.RandomState(seed=RANDOM_STATE).shuffle(idxs_minor_train)\n",
    "        \n",
    "        # For each mini-batch\n",
    "        for i in range(n_batch):\n",
    "            # Get the first and last index (exclusive) of the mini-batch\n",
    "            first_idx = i * batch_size\n",
    "            last_idx = min((i + 1) * batch_size, len(idxs_minor_train))                         \n",
    "            # Get the mini-batch\n",
    "            mb = idxs_minor_train[first_idx : last_idx]\n",
    "            # Get the real feature matrix\n",
    "            real_features = X_minor_train[mb, :]\n",
    "            # Get the noise\n",
    "            noise = tf.random.normal(shape=[len(mb), coding_size], seed=RANDOM_STATE)\n",
    "            # Get the gen feature matrix\n",
    "            gen_features = generator(noise)\n",
    "            # Combine the generated and real feature matrix\n",
    "            gen_real_features = tf.concat([gen_features, real_features], axis=0)\n",
    "            # Get the target vector\n",
    "            y = tf.constant([[0.]] * len(mb) + [[1.]] * len(mb))\n",
    "            # Unfreeze the discriminator\n",
    "            discriminator.trainable = True\n",
    "            # Train the discriminator\n",
    "            discriminator.train_on_batch(gen_real_features, y)\n",
    "            # Get the noise\n",
    "            noise = tf.random.normal(shape=[len(mb), coding_size], seed=RANDOM_STATE)\n",
    "            # Get the target\n",
    "            y = tf.constant([[1.]] * len(mb))\n",
    "            # Freeze the discriminator\n",
    "            discriminator.trainable = False\n",
    "            # Train the generator\n",
    "            gan.train_on_batch(noise, y)\n",
    "\n",
    "    # Return GANs after training\n",
    "    return gan\n",
    "\n",
    "\n",
    "\n",
    "def resampling_dataset(X, y, over_tech, verbose=True):\n",
    "    if over_tech == 'none': # Apply only Random Undersampling (RUS)\n",
    "        if verbose: print('Undersampling: Random Undersampling')\n",
    "        return RandomUnderSampler(random_state=RANDOM_STATE).fit_resample(X, y)\n",
    "\n",
    "    # Combine the Undersampling and Oversampling mmethods\n",
    "    # Obtain the number of samples for each class\n",
    "    value_counts = pd.Series(y).value_counts(ascending=True)\n",
    "    # Mean number of samples from minority classes\n",
    "    mean_minority_classes = int(np.mean(value_counts[:-1].values))\n",
    "    # Dictionary  that will be used for resampling\n",
    "    sampling_strategy = dict()\n",
    "\n",
    "    # Build sampling_strategy dictionary\n",
    "    for i in range(len(value_counts)):\n",
    "        if value_counts[i] >= mean_minority_classes: # If there are more than the mean number\n",
    "            sampling_strategy[i] = mean_minority_classes\n",
    "        else: # The value is not updated\n",
    "            sampling_strategy[i] = value_counts[i]\n",
    "\n",
    "    # Undersampling\n",
    "    rus = RandomUnderSampler(sampling_strategy=sampling_strategy, \n",
    "                             random_state=RANDOM_STATE)\n",
    "    X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "    if verbose: print('Undersampling: Random Undersampling')\n",
    "    \n",
    "    # List to save classes that will be applied oversampling\n",
    "    over_classes = []\n",
    "    # Update sampling_strategy values ​​after RUS\n",
    "    for i in range(len(value_counts)):\n",
    "        if value_counts[i] < mean_minority_classes: # Add the number of samples for oversampling\n",
    "            sampling_strategy[i] = mean_minority_classes\n",
    "            over_classes.append(i)\n",
    "\n",
    "    # Oversampling\n",
    "    if over_tech == 'ros':\n",
    "        # Random Oversampling (ROS)\n",
    "        ros = RandomOverSampler(sampling_strategy=sampling_strategy, \n",
    "                                random_state=RANDOM_STATE)\n",
    "        X_resampled, y_resampled = ros.fit_resample(X_resampled, y_resampled)\n",
    "        if verbose: print('Oversampling: Random Oversampling')\n",
    "    elif over_tech == 'sm':\n",
    "        # SMOTE (SM)\n",
    "        sm = SMOTE(sampling_strategy=sampling_strategy, \n",
    "                   random_state=RANDOM_STATE, n_jobs=-1)\n",
    "        X_resampled, y_resampled = sm.fit_resample(X_resampled, y_resampled)\n",
    "        if verbose: print('Oversampling: SMOTE')\n",
    "    elif over_tech == 'k-sm':\n",
    "        # K-Means SMOTE (K-SM)\n",
    "        k_sm = KMeansSMOTE(cluster_balance_threshold='auto', sampling_strategy=sampling_strategy, \n",
    "                           random_state=RANDOM_STATE, n_jobs=-1)\n",
    "        X_resampled, y_resampled = k_sm.fit_resample(X_resampled, y_resampled)\n",
    "        if verbose: print('Oversampling: K-Means SMOTE')\n",
    "    elif over_tech == 'gans':\n",
    "        # Generative Adversarial Networks (GANs)\n",
    "        # Set the number of features\n",
    "        n_features = X_resampled.shape[1]\n",
    "        # Set the coding size, which is the dimension of the noise used as input for the generator\n",
    "        coding_size = n_features // 2\n",
    "        # Convert to NumPy arrays\n",
    "        y_resampled = y_resampled.values\n",
    "        X_resampled = X_resampled.to_numpy()\n",
    "\n",
    "        for i in range(len(over_classes)): # For each class that will be applied oversampling \n",
    "            # Identifying the minority class\n",
    "            minor_class = over_classes[i]\n",
    "            # Get the training feature matrix of the minority class\n",
    "            X_minor_train = X_resampled[np.where(y_resampled == minor_class)]\n",
    "            # Get the training target vector of the minority class\n",
    "            y_minor_train = y_resampled[np.where(y_resampled == minor_class)]\n",
    "            # Generate trained GANs\n",
    "            gan = generate_gans(X_minor_train, n_features, coding_size)\n",
    "            # Get the generator\n",
    "            generator = gan.layers[0]\n",
    "            # Get the number of samples to generate\n",
    "            n_class_diff = mean_minority_classes - y_minor_train.shape[0]\n",
    "            # Initialize the generated data\n",
    "            gen_data = np.zeros((n_class_diff, X_minor_train.shape[1] + 1))\n",
    "\n",
    "            for i in range(n_class_diff):\n",
    "                # Get the noise\n",
    "                noise = tf.random.normal(shape=[1, coding_size], seed=RANDOM_STATE)\n",
    "                # Get the generated features\n",
    "                gen_features = generator(noise)\n",
    "                # Update the generated data\n",
    "                gen_data[i, :-1], gen_data[i, -1] = gen_features, minor_class\n",
    "\n",
    "            # Augment the minority class in the training data\n",
    "            # Augment the training feature matrix\n",
    "            X_resampled = np.vstack((X_resampled, gen_data[:, :-1]))\n",
    "            # Augment the training target vector\n",
    "            y_resampled = np.vstack((y_resampled.reshape(-1, 1), gen_data[:, -1].reshape(-1, 1))).reshape(-1)\n",
    "            \n",
    "        # Convert to Pandas DataFrame and Series\n",
    "        X_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "        y_resampled = pd.Series(y_resampled).astype(int)\n",
    "        if verbose: print('Oversampling: Generative Adversarial Networks')\n",
    "    else:\n",
    "        if verbose: print('Oversampling: Error!')\n",
    "\n",
    "    # X and y after resampling the data\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztUkAcPS0_IE"
   },
   "outputs": [],
   "source": [
    "# Main\n",
    "\n",
    "# Dataset path\n",
    "dataset_path = '/home/leandro/remy-project/centralized/datasets/WSN-DS/'\n",
    "\n",
    "# Load dataset\n",
    "wsn_df = pd.read_csv(f'{dataset_path}data.csv')\n",
    "# Sort DataFrame by 'Time' column\n",
    "wsn_df = wsn_df.sort_values(by=[' Time']).reset_index(drop=True)\n",
    "# Drop 'id' column\n",
    "wsn_df.drop(columns=' id', inplace=True)\n",
    "# Rename class column\n",
    "wsn_df.rename(columns={\"Attack type\": \"class\"}, inplace=True)\n",
    "# Remove TDMA schedule attack\n",
    "wsn_df = wsn_df[wsn_df['class'] != 'TDMA'].reset_index(drop=True)\n",
    "\n",
    "# Convert classes to numeric\n",
    "wsn_df[\"class\"] = wsn_df[\"class\"].map({\n",
    "    \"Normal\": 0,\n",
    "    \"Grayhole\": 1,\n",
    "    \"Blackhole\": 2,\n",
    "    \"Flooding\": 3\n",
    "}.get)\n",
    "\n",
    "# Normalize the data \n",
    "wsn_df = normalize_data(wsn_df)\n",
    "# Split the data into 60% for training and 40% for test\n",
    "train_df, test_df = split_train_test(wsn_df, 0.4) \n",
    "# Save test dataset in .csv\n",
    "test_df.to_csv(f'{dataset_path}test_data.csv', index=False)\n",
    "# Split train_df into X and y\n",
    "X_train, y_train = split_x_y(train_df)\n",
    "\n",
    "# Balance training data with various techniques\n",
    "for oversampling_tech in ['none', 'ros', 'sm', 'k-sm', 'gans']:\n",
    "    # Balance data\n",
    "    X_res, y_res = resampling_dataset(X_train, y_train, oversampling_tech, verbose=False)\n",
    "    X_res['class'] = y_res.values # Add class column in X_res DataFrame\n",
    "    # Save balanced dataset in .csv\n",
    "    X_res.to_csv(f'{dataset_path}balanced/data_{oversampling_tech}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwmuQAwfDLzJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOnjJWB1Ppkr2QqrP+c9FS4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
